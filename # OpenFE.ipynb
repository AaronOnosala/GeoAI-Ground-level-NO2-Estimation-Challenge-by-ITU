{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e747bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('')\n",
    "\n",
    "# Load files\n",
    "train = pd.read_csv(DATA_PATH / '/Users/aarononosala/Downloads/geoai-ground-level-no2-estimation-challenge20240612-4943-16iro0r/Train.csv')\n",
    "test = pd.read_csv(DATA_PATH / '/Users/aarononosala/Downloads/geoai-ground-level-no2-estimation-challenge20240612-4943-16iro0r/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.dropna()\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date']) \n",
    "data['year'] = data['Date'].dt.year\n",
    "data['month'] = data['Date'].dt.month\n",
    "data['day'] = data['Date'].dt.day\n",
    "\n",
    "data.drop(['ID_Zindi', 'ID','Date'], axis=1, inplace = True)\n",
    "\n",
    "target = data[['GT_NO2']]\n",
    "train_data = data.drop(target, axis=1)\n",
    "\n",
    "test['Date'] = pd.to_datetime(test['Date']) \n",
    "test['year'] = test['Date'].dt.year\n",
    "test['month'] = test['Date'].dt.month\n",
    "test['day'] = test['Date'].dt.day\n",
    "\n",
    "test.drop(['ID_Zindi', 'ID', 'Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape , target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfe import OpenFE, tree_to_formula, transform, get_candidate_features\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_jobs = 52\n",
    "    params = {\"n_estimators\": 1000, \"importance_type\": \"gain\", \"num_leaves\": 64,\n",
    "               \"seed\": 1, \"n_jobs\": n_jobs}\n",
    "\n",
    "    ofe1 = OpenFE()\n",
    "    candidate_features_list = get_candidate_features(numerical_features=list(test.columns))\n",
    "\n",
    "    features1 = ofe1.fit(data=train_data, label=target,\n",
    "                        candidate_features_list=candidate_features_list, metric='rmse', task='regression', stage2_params=params,\n",
    "                        min_candidate_features=5000,\n",
    "                        n_jobs=n_jobs, n_data_blocks=2, feature_boosting=True)\n",
    "\n",
    "    train_ft1, test_ft1 = transform(train_data, test, features1[:300], n_jobs=n_jobs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5aa69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final = pd.concat([train_data, train_ft1], axis=1) \n",
    "test_final = pd.concat([test, test_ft1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54182428",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.shape , test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7660c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def preprocess_data(X):\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    return X\n",
    "\n",
    "def get_most_important_features(X_train, y_train, n, model_input, device='cpu'):\n",
    "    xgb_params = {\n",
    "        'n_jobs': -1,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': 'hist',\n",
    "        'verbosity': 0,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "    if device == 'gpu':\n",
    "        xgb_params['tree_method'] = 'gpu_hist'\n",
    "        xgb_params['predictor'] = 'gpu_predictor'\n",
    "        \n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'device': device,\n",
    "        'n_jobs':-1\n",
    "    }\n",
    "    \n",
    "    cb_params = {\n",
    "        'grow_policy': 'Depthwise',\n",
    "        'bootstrap_type': 'Bayesian',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'loss_function': 'RMSE',\n",
    "        'random_state': 42,\n",
    "        'task_type': device.upper(),\n",
    "        'thread_count':-1\n",
    "    }\n",
    "    \n",
    "    if 'xgb' in model_input:\n",
    "        model = xgb.XGBRegressor(**xgb_params)\n",
    "    elif 'cat' in model_input:\n",
    "        model = CatBoostRegressor(**cb_params)\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(**lgb_params)\n",
    "    \n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    \n",
    "    rmse_scores = []\n",
    "    feature_importances_list = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        for train_idx, val_idx in tqdm(kfold.split(X_train)):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "            X_train_fold = preprocess_data(X_train_fold)\n",
    "            X_val_fold = preprocess_data(X_val_fold)\n",
    "            \n",
    "            if 'lgb' in model_input:\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "            else:\n",
    "                model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "            \n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            rmse_scores.append(np.sqrt(mean_squared_error(y_val_fold, y_pred)))\n",
    "            \n",
    "            feature_importances = model.feature_importances_\n",
    "            feature_importances_list.append(feature_importances)\n",
    "    \n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    avg_feature_importances = np.mean(feature_importances_list, axis=0)\n",
    "    \n",
    "    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(avg_feature_importances)]\n",
    "    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_n_features = [feature[0] for feature in sorted_features[:n]]\n",
    "    display_features = top_n_features[:10]\n",
    "    \n",
    "    sns.set_palette(\"Set2\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(range(len(display_features)), [avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features])\n",
    "    plt.yticks(range(len(display_features)), display_features, fontsize=12)\n",
    "    plt.xlabel('Average Feature Importance', fontsize=14)\n",
    "    plt.ylabel('Features', fontsize=10)\n",
    "    plt.title(f'Top {10} of {n} Feature Importances with RMSE score {avg_rmse:.3f}', fontsize=16)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "\n",
    "    # Add data labels on the bars\n",
    "    for index, value in enumerate([avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features]):\n",
    "        plt.text(value + 0.005, index, f'{value:.3f}', fontsize=12, va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_n_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b77759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate column names\n",
    "duplicate_columns = train_final.columns[train_final.columns.duplicated()].unique()\n",
    "if len(duplicate_columns) > 0:\n",
    "    print(f\"Duplicate columns found: {duplicate_columns}\")\n",
    "    # Option 1: Rename duplicate columns\n",
    "    #train_final = train_final.rename(columns=lambda x: x + '_dup' if x in duplicate_columns else x)\n",
    "    \n",
    "    # Option 2: Drop duplicate columns (if they're not needed)\n",
    "    train_final = train_final.loc[:, ~train_final.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_imp_features_cat=get_most_important_features(train_final.reset_index(drop=True), target,10, 'cat')\n",
    "#n_imp_features_xgb=get_most_important_features(train_final.reset_index(drop=True), target,30, 'xgb')\n",
    "#n_imp_features_lgbm=get_most_important_features(train_final.reset_index(drop=True), target,30, 'lgbm')\n",
    "\n",
    "#n_imp_features=[*set(n_imp_features_xgb+n_imp_features_lgbm+n_imp_features_cat)]\n",
    "print(f\"{len(n_imp_features_cat)} features have been selected from three algorithms for the final model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a7e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(n_imp_features_cat)} features have been selected from three algorithms for the final model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([train_final[n_imp_features]] + target, axis=1)\n",
    "#test1 = test_final[n_imp_features]\n",
    "\n",
    "df = pd.concat([train_final[n_imp_features_cat]] + target, axis=1)\n",
    "test1 = test_final[n_imp_features_cat]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
